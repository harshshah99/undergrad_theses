@article{Rumelhart1986LearningRB,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536}
}

@article{Rosenblatt1958ThePA,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Frank Rosenblatt},
  journal={Psychological review},
  year={1958},
  volume={65 6},
  pages={
          386-408
        }
}

@article{kjones_tfidf,
author = {Jones, K.},
year = {2004},
month = {01},
pages = {493-502},
title = {A Statistical Interpretation of Term Specificity in Retrieval},
volume = {60},
journal = {Journal of Documentation},
doi = {10.1108/00220410410560573}
}

@inbook{manning_raghavan_schütze_2008, place={Cambridge}, title={Scoring, term weighting, and the vector space model}, DOI={10.1017/CBO9780511809071.007}, booktitle={Introduction to Information Retrieval}, publisher={Cambridge University Press}, author={Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich}, year={2008}, pages={100–123}}

@article{lstm_schmidhuber,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}

@article{10.1162/neco.1997.9.8.1735,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = {nov},
pages = {1735–1780},
numpages = {46}
}

@article{Gers2000RecurrentNT,
  title={Recurrent nets that time and count},
  author={Felix A. Gers and J{\"u}rgen Schmidhuber},
  journal={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},
  year={2000},
  volume={3},
  pages={189-194 vol.3}
}

@article{Sung2017EmployeesRT,
  title={Employees’ Responses to an Organizational Merger: Intraindividual Change in Organizational Identification, Attachment, and Turnover},
  author={Wookje Sung and Meredith Woehler and Jesse M. Fagan and Travis J. Grosser and Theresa M Floyd and Giuseppe Joe Labianca},
  journal={Journal of Applied Psychology},
  year={2017},
  volume={102},
  pages={910–934}
}

@article{pennebaker2007linguistic,
  title={Linguistic inquiry and word count: {LIWC} },
  author={Pennebaker, James W and Booth, Roger J and Francis, Martha E},
  year={2007}
}

@unknown{liwc2015,
author = {Pennebaker, James and Boyd, Ryan and Jordan, Kayla and Blackburn, Kate},
year = {2015},
month = {09},
pages = {},
title = {The Development and Psychometric Properties of LIWC2015},
doi = {10.15781/T29G6Z}
}

@article{niculae2015linguistic,
  title={Linguistic harbingers of betrayal: A case study on an online strategy game},
  author={Niculae, Vlad and Kumar, Srijan and Boyd-Graber, Jordan and Danescu-Niculescu-Mizil, Cristian},
  journal={arXiv preprint arXiv:1506.04744},
  year={2015}
}

@inproceedings{chang-etal-2020-convokit,
    title = "{C}onvo{K}it: A Toolkit for the Analysis of Conversations",
    author = "Chang, Jonathan P.  and
      Chiam, Caleb  and
      Fu, Liye  and
      Wang, Andrew  and
      Zhang, Justine  and
      Danescu-Niculescu-Mizil, Cristian",
    booktitle = "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = jul,
    year = "2020",
    address = "1st virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.sigdial-1.8",
    pages = "57--60",
    abstract = "This paper describes the design and functionality of ConvoKit, an open-source toolkit for analyzing conversations and the social interactions embedded within. ConvoKit provides an unified framework for representing and manipulating conversational data, as well as a large and diverse collection of conversational datasets. By providing an intuitive interface for exploring and interacting with conversational data, this toolkit lowers the technical barriers for the broad adoption of computational methods for conversational analysis.",
}

@inproceedings{DanescuNiculescuMizil2013ACA,
  title={A computational approach to politeness with application to social factors},
  author={Cristian Danescu-Niculescu-Mizil and Moritz Sudhof and Dan Jurafsky and Jure Leskovec and Christopher Potts},
  booktitle={ACL},
  year={2013}
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@article{cnn_726791,  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},  journal={Proceedings of the IEEE},   title={Gradient-based learning applied to document recognition},   year={1998},  volume={86},  number={11},  pages={2278-2324},  doi={10.1109/5.726791}}

@inproceedings{gu-budhkar-2021-package,
    title = "A Package for Learning on Tabular and Text Data with Transformers",
    author = "Gu, Ken  and
      Budhkar, Akshay",
    booktitle = "Proceedings of the Third Workshop on Multimodal Artificial Intelligence",
    month = jun,
    year = "2021",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.maiworkshop-1.10",
    doi = "10.18653/v1/2021.maiworkshop-1.10",
    pages = "69--73",
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@misc{mukherjee2019contentbased,
      title={A Content-Based Approach to Email Triage Action Prediction: Exploration and Evaluation}, 
      author={Sudipto Mukherjee and Ke Jiang},
      year={2019},
      eprint={1905.01991},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{10.1145/3077136.3080782,
author = {Yang, Liu and Dumais, Susan T. and Bennett, Paul N. and Awadallah, Ahmed Hassan},
title = {Characterizing and Predicting Enterprise Email Reply Behavior},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080782},
doi = {10.1145/3077136.3080782},
abstract = {Email is still among the most popular online activities. People spend a significant amount of time sending, reading and responding to email in order to communicate with others, manage tasks and archive personal information. Most previous research on email is based on either relatively small data samples from user surveys and interviews, or on consumer email accounts such as those from Yahoo! Mail or Gmail. Much less has been published on how people interact with enterprise email even though it contains less automatically generated commercial email and involves more organizational behavior than is evident in personal accounts. In this paper, we extend previous work on predicting email reply behavior by looking at enterprise settings and considering more than dyadic communications. We characterize the influence of various factors such as email content and metadata, historical interaction features and temporal features on email reply behavior. We also develop models to predict whether a recipient will reply to an email and how long it will take to do so. Experiments with the publicly-available Avocado email collection show that our methods outperform all baselines with large gains. We also analyze the importance of different features on reply behavior predictions. Our findings provide new insights about how people interact with enterprise email and have implications for the design of the next generation of email clients.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {235–244},
numpages = {10},
keywords = {user behavior modeling, information overload, email reply behavior},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{36955,
title	= {The Learning Behind Gmail Priority Inbox},
author	= {Douglas Aberdeen and Ondrey Pacovsky and Andrew Slater},
year	= {2010},
booktitle	= {LCCC  :  NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds}
}



@article{maya_email_intent,
author = {Sappelli, Maya and Pasi, Gabriella and Verberne, Suzan and de Boer, Maaike and Kraaij, Wessel},
year = {2016},
month = {03},
pages = {},
title = {Assessing e-mail intent and tasks in e-mail messages},
volume = {358},
journal = {Information Sciences},
doi = {10.1016/j.ins.2016.03.002}
}
@inproceedings{Wu2020GraphCN,
  title={Graph Convolutional Networks with Markov Random Field Reasoning for Social Spammer Detection},
  author={Yongji Wu and Defu Lian and Yiheng Xu and Le Wu and Enhong Chen},
  booktitle={AAAI},
  year={2020}
}
@phdthesis{fout2017protein,
  title={Protein interface prediction using graph convolutional networks},
  author={Fout, Alex M},
  year={2017},
  school={Colorado State University}
}

@inproceedings{kipf2017semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N. and Welling, Max},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}
@inproceedings{Fey/Lenssen/2019,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}



@inproceedings{DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{StellarGraph,
  author = {CSIRO's Data61},
  title = {StellarGraph Machine Learning Library},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/stellargraph/stellargraph}},
}

@article{
  velickovic2018graph,
  title="{Graph Attention Networks}",
  author={Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
  journal={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=rJXMpikCZ},
}

@inproceedings{DBLP:conf/ecml/KlimtY04,
  author    = {Bryan Klimt and
               Yiming Yang},
  editor    = {Jean{-}Fran{\c{c}}ois Boulicaut and
               Floriana Esposito and
               Fosca Giannotti and
               Dino Pedreschi},
  title     = {The Enron Corpus: {A} New Dataset for Email Classification Research},
  booktitle = {Machine Learning: {ECML} 2004, 15th European Conference on Machine
               Learning, Pisa, Italy, September 20-24, 2004, Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {3201},
  pages     = {217--226},
  publisher = {Springer},
  year      = {2004},
  url       = {https://doi.org/10.1007/978-3-540-30115-8\_22},
  doi       = {10.1007/978-3-540-30115-8\_22},
  timestamp = {Tue, 14 May 2019 10:00:54 +0200},
  biburl    = {https://dblp.org/rec/conf/ecml/KlimtY04.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3289600.3291028,
author = {Sarrafzadeh, Bahareh and Hassan Awadallah, Ahmed and Lin, Christopher H. and Lee, Chia-Jung and Shokouhi, Milad and Dumais, Susan T.},
title = {Characterizing and Predicting Email Deferral Behavior},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3291028},
doi = {10.1145/3289600.3291028},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {627–635},
numpages = {9},
keywords = {triage, intelligent assistant, email management, task management, users behavior modeling, deferral, mixed methods approaches, intelligent email client},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{alkhereyf-rambow-2017-work,
    title = "Work Hard, Play Hard: Email Classification on the Avocado and {E}nron Corpora",
    author = "Alkhereyf, Sakhar  and
      Rambow, Owen",
    booktitle = "Proceedings of {T}ext{G}raphs-11: the Workshop on Graph-based Methods for Natural Language Processing",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-2408",
    doi = "10.18653/v1/W17-2408",
    pages = "57--65",
    abstract = "In this paper, we present an empirical study of email classification into two main categories {``}Business{''} and {``}Personal{''}. We train on the Enron email corpus, and test on the Enron and Avocado email corpora. We show that information from the email exchange networks improves the performance of classification. We represent the email exchange networks as social networks with graph structures. For this classification task, we extract social networks features from the graphs in addition to lexical features from email content and we compare the performance of SVM and Extra-Trees classifiers using these features. Combining graph features with lexical features improves the performance on both classifiers. We also provide manually annotated sets of the Avocado and Enron email corpora as a supplementary contribution.",
}

@article{liu2019roberta,
    title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and
              Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and
              Luke Zettlemoyer and Veselin Stoyanov},
    journal={arXiv preprint arXiv:1907.11692},
    year = {2019},
}


@misc{graves2014neural,
      title={Neural Turing Machines}, 
      author={Alex Graves and Greg Wayne and Ivo Danihelka},
      year={2014},
      eprint={1410.5401},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@article{Bahdanau2015NeuralMT,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal={CoRR},
  year={2015},
  volume={abs/1409.0473}
}


@inproceedings{luong-etal-2015-effective,
    title = "Effective Approaches to Attention-based Neural Machine Translation",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1166",
    doi = "10.18653/v1/D15-1166",
    pages = "1412--1421",
}


@inproceedings{10.5555/3295222.3295349,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is All You Need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


% blog and other sources to cite

% https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks
% https://alexlenail.me/NN-SVG/ - fig 2.1
% https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks
%  
